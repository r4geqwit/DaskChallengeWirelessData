{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DaskChallengeSupervisedLearningModels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNOU2d2ZmSPU"
      },
      "source": [
        "#**Purpose of this Analysis**\n",
        "\n",
        "\n",
        "The dataset consists of customer information over the month of January 2020 for a company. The purpose of this analysis is to determine the key features that lead to which wireless carrier employees will likely select. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo4ijLI5xVRK"
      },
      "source": [
        "#Build Dask environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSnXAIPCwuw6",
        "outputId": "76bb915d-aeca-4492-be3f-ac57fd6e1a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install --upgrade 'dask[complete]'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dask[complete]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/94/b4012c61c09300f4413c58a522a6cc1a212dc4a7f6fe1ba98d67429c089d/dask-2.30.0-py3-none-any.whl (848kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (3.13)\n",
            "Collecting fsspec>=0.6.0; extra == \"complete\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.4MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10; extra == \"complete\"\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
            "Collecting distributed>=2.0; extra == \"complete\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/39/665a3b952ac889839eead70813dbc638fe49be4c6217ed249fc06b6485fa/distributed-2.30.0-py3-none-any.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: toolz>=0.8.2; extra == \"complete\" in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=0.2.2; extra == \"complete\" in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: bokeh!=2.0.0,>=1.0.0; extra == \"complete\" in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.23.0; extra == \"complete\" in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.0; extra == \"complete\" in /usr/local/lib/python3.6/dist-packages (from dask[complete]) (1.18.5)\n",
            "Collecting locket\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/22/3c0f97614e0be8386542facb3a7dcfc2584f7b83608c02333bced641281c/locket-0.2.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (5.4.8)\n",
            "Collecting contextvars; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0; extra == \"complete\"->dask[complete]) (2018.9)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.0; extra == \"complete\"->dask[complete]) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh!=2.0.0,>=1.0.0; extra == \"complete\"->dask[complete]) (2.4.7)\n",
            "Building wheels for collected packages: locket, contextvars\n",
            "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for locket: filename=locket-0.2.0-cp36-none-any.whl size=4040 sha256=44113bcb111d6818a6d9389cb78e060ad8a11870a1f151f2b63ac1d021b85e74\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/1e/e8/4fa236ec931b1a0cdd61578e20d4934d7bf188858723b84698\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=a772a0ca488fe4750f3b82aa22d16868e19fd87ddf3b69bb7344a57eb794973a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built locket contextvars\n",
            "\u001b[31mERROR: distributed 2.30.0 has requirement cloudpickle>=1.5.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fsspec, locket, partd, immutables, contextvars, distributed, dask\n",
            "  Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "Successfully installed contextvars-2.4 dask-2.30.0 distributed-2.30.0 fsspec-0.8.4 immutables-0.14 locket-0.2.0 partd-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91yvkXVVEf-Y"
      },
      "source": [
        "#Build ML library for Dask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeVMy2cXEkpH",
        "outputId": "6eff4782-f221-4eea-925a-82cc8627370d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pip install dask-ml"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dask-ml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/b0/3d4b11fd2468dd098ab6ef2a5a094192daddb812c11da948b9f318a46073/dask_ml-1.7.0-py3-none-any.whl (141kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n",
            "\u001b[?25hCollecting multipledispatch>=0.4.9\n",
            "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from dask-ml) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from dask-ml) (20.4)\n",
            "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from dask-ml) (2.30.0)\n",
            "Collecting dask-glm>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/ee/36c6e0e7b51e08406e5c3bb036f35adb77bd0a89335437b2e6f03c948f1a/dask_glm-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from dask-ml) (0.48.0)\n",
            "Collecting scikit-learn>=0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from dask-ml) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.6/dist-packages (from dask-ml) (1.18.5)\n",
            "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from dask-ml) (2.30.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from multipledispatch>=0.4.9->dask-ml) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->dask-ml) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->dask-ml) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->dask-ml) (2.4.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (3.13)\n",
            "Requirement already satisfied: toolz>=0.8.2; extra == \"array\" in /usr/local/lib/python3.6/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.11.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.8.4)\n",
            "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from dask-glm>=0.2.0->dask-ml) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->dask-ml) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->dask-ml) (0.31.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.23->dask-ml) (0.16.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (5.4.8)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (2.2.2)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (5.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (7.1.2)\n",
            "Requirement already satisfied: contextvars; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.4.0->dask-ml) (2.4)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.6/dist-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[array,dataframe]>=2.4.0->dask-ml) (0.2.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version < \"3.7\"->distributed>=2.4.0->dask-ml) (0.14)\n",
            "Installing collected packages: multipledispatch, threadpoolctl, scikit-learn, dask-glm, dask-ml\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed dask-glm-0.2.0 dask-ml-1.7.0 multipledispatch-0.6.0 scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmpkrhohuXkK"
      },
      "source": [
        "#**Restart Runtime and Continue**\n",
        "\n",
        "Restart required in order to apply several packages that were installed to this active runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbvieQV_GPG-"
      },
      "source": [
        "#Install Aiohttp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmRkGHN1GKJD",
        "outputId": "90e3fa20-5d8b-4cd9-b15c-ced20f7fcd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install aiohttp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/a6/93acfa8f8b3573c4445ace2f266de62783231923706a4c3ab705e7d43497/aiohttp-3.6.3-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 3.5MB/s \n",
            "\u001b[?25hCollecting yarl<1.6.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b4/2cbeaf2c3ea53865d9613b315fe24e78c66acedb1df7e4be4e064c87203b/yarl-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (257kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp) (3.7.4.3)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 20.5MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp) (20.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<1.6.0,>=1.0->aiohttp) (2.10)\n",
            "Building wheels for collected packages: idna-ssl\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=ab6520f3f9a2bafba6784df57bb089630df6a2e8fa85387c83e1612164330363\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built idna-ssl\n",
            "Installing collected packages: multidict, yarl, idna-ssl, async-timeout, aiohttp\n",
            "Successfully installed aiohttp-3.6.3 async-timeout-3.0.1 idna-ssl-1.1.0 multidict-4.7.6 yarl-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAkQVxa2oRwB"
      },
      "source": [
        "#Client Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UO6Im3vxc9n",
        "outputId": "4d3538c2-1398-438e-efd9-57d3dc5f10e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from dask.distributed import Client, progress\n",
        "\n",
        "client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
        "client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table style=\"border: 2px solid white;\">\n",
              "<tr>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3 style=\"text-align: left;\">Client</h3>\n",
              "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
              "  <li><b>Scheduler: </b>tcp://127.0.0.1:38637</li>\n",
              "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
              "</ul>\n",
              "</td>\n",
              "<td style=\"vertical-align: top; border: 0px solid white\">\n",
              "<h3 style=\"text-align: left;\">Cluster</h3>\n",
              "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
              "  <li><b>Workers: </b>4</li>\n",
              "  <li><b>Cores: </b>8</li>\n",
              "  <li><b>Memory: </b>8.00 GB</li>\n",
              "</ul>\n",
              "</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<Client: 'tcp://127.0.0.1:38637' processes=4 threads=8, memory=8.00 GB>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaLq2BxVxlgB",
        "outputId": "672f8e16-925d-410b-9631-373040e48e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 8787 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 21:12:07--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.71.209.190, 34.205.198.58, 52.2.56.23, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.71.209.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  40.3MB/s    in 0.3s    \n",
            "\n",
            "2020-10-18 21:12:08 (40.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxGYlKmEAgc6"
      },
      "source": [
        "#Load Data\n",
        "\n",
        "Data loaded into a Dask dataframe, dropping columns that will not be used in analysis, and applying dummy encoder to target variable(Carrier). In this case the taret was an object type so conversion was fairly simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Bt7nINAiSH"
      },
      "source": [
        "import math\n",
        "from scipy import stats\n",
        "import datetime as dt\n",
        "import time\n",
        "\n",
        "from dask import delayed\n",
        "import dask.dataframe as dd\n",
        "from dask_ml.preprocessing import DummyEncoder, Categorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from dask_ml.decomposition import PCA\n",
        "import dask.array as da\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import joblib\n",
        "from dask_ml.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agxICABoAsCt"
      },
      "source": [
        "capstone = dd.read_csv('https://raw.githubusercontent.com/r4geqwit/WirelessData/master/RawDataJanuary.csv', dtype={'Carrier': 'category'},assume_missing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcY0UUK4A2n2"
      },
      "source": [
        "df = capstone.drop(['Month', 'Plans_1', 'Employee Number', 'Year'], axis=1)\n",
        "pipe = make_pipeline(\n",
        "    Categorizer(), DummyEncoder(drop_first=True))\n",
        "df1 = pipe.fit_transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28QkBRSOOgC7"
      },
      "source": [
        "#Create Train and Test splits\n",
        "\n",
        "Aside from some changes here for use with Dask, the models all run the same with similar results to Pandas and Numpy variation of this model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Tt5_oXNw9J",
        "outputId": "e2ed7fd3-5516-46cb-d9ed-e43ebef14f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = df1.drop('Carrier_Verizon US', axis=1)\n",
        "y = df1['Carrier_Verizon US']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train.persist()\n",
        "X_test.persist()\n",
        "y_train.persist()\n",
        "y_test.persist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dask Series Structure:\n",
              "npartitions=1\n",
              "    uint8\n",
              "      ...\n",
              "Name: Carrier_Verizon US, dtype: uint8\n",
              "Dask Name: split, 1 tasks"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnco1mRHPvBO"
      },
      "source": [
        "#Train and test Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1z56q5OP2yh"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz1CybZVP8Pv",
        "outputId": "d92cb975-8e21-48ee-aedd-42fef8838a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = LogisticRegression(solver='sag', penalty='l2', max_iter=1000, random_state=1)\n",
        "with joblib.parallel_backend('dask'):\n",
        "  lr.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = lr.predict(X_train.values.compute())\n",
        "preds_test = lr.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.8486011057128497\n",
            "Test score is:  0.8754578754578755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXlb08vNSKqM"
      },
      "source": [
        "#Train and test KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi9AHTurSOuS"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6yNVyN8SasJ",
        "outputId": "17dd83a7-590f-4d74-b815-3eb10e76d774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=8)\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  knn.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = knn.predict(X_train.values.compute())\n",
        "preds_test = knn.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.7141466750841751\n",
            "Test score is:  0.5327272727272727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzARtgMQS2mG"
      },
      "source": [
        "#Train and test SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y9osImHS7ys"
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbpCzuXxTAqA",
        "outputId": "80e320e4-3126-44e4-8454-eae18bf09b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time_start = time.time()\n",
        "\n",
        "svc = svm.SVC(kernel = 'linear', random_state=1)\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  svc.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = svc.predict(X_train.values.compute())\n",
        "preds_test = svc.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))\n",
        "\n",
        "print('Time elapsed: {} seconds'.format(time.time()-time_start)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.9673898475456526\n",
            "Test score is:  0.9680851063829787\n",
            "Time elapsed: 98.0911934375763 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3AgFnGlQMGZ"
      },
      "source": [
        "#Train and test Decision Tree model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifyfv1LzQQYA"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrB8ghZjQiKy",
        "outputId": "585472a9-a788-44cd-fb08-82e447068af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dtc = DecisionTreeClassifier(\n",
        "     criterion='entropy',\n",
        "    max_features=5,\n",
        "    max_depth=5,\n",
        "    random_state=1 \n",
        ")\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  dtc.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = dtc.predict(X_train.values.compute())\n",
        "preds_test = dtc.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.9801671197020034\n",
            "Test score is:  0.9791666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RumpzaLbQsrP"
      },
      "source": [
        "#Train and test Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xey78MxTQwTm"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km048_h0Q4LW",
        "outputId": "82a360dd-f7e1-4a96-ebfe-a41ad5ea406d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
        "with joblib.parallel_backend('dask'):\n",
        "  rfc.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = rfc.predict(X_train.values.compute())\n",
        "preds_test = rfc.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  1.0\n",
            "Test score is:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3oUxhhJRi8f"
      },
      "source": [
        "#Test and train Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu8w6wXZRnNp"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK-5Tg4gRq7y",
        "outputId": "b9b4aba3-cb0e-4d6a-a0c4-08753b8d4e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=1000, \n",
        "    max_depth=5, \n",
        "    loss='deviance', \n",
        "    random_state=1\n",
        "    )\n",
        "with joblib.parallel_backend('dask'):\n",
        "  gbc.fit(X_train.values.compute(), y_train.values.compute())\n",
        "\n",
        "preds_train = gbc.predict(X_train.values.compute())\n",
        "preds_test = gbc.predict(X_test.values.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  1.0\n",
            "Test score is:  0.9893617021276595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0EI0I4K0gPr"
      },
      "source": [
        "#Convert features into Dask array, then apply Dask ml PCA to training data\n",
        "\n",
        "The challenge here was figuring out to convert the target variable into an array. After that was done, I chose to grab the top 20 prinicipal components of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc_1Rjcd0i3q"
      },
      "source": [
        "X = df1.drop('Carrier_Verizon US', axis=1)\n",
        "y = df1['Carrier_Verizon US']\n",
        "\n",
        "X = X.to_dask_array(lengths=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGp4-BCj6SdW"
      },
      "source": [
        "pca = PCA(n_components=20, svd_solver='tsqr')\n",
        "pca.fit(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, convert_mixed_types=True)\n",
        "\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5l1NEvL6c2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj50NDlgYq5x"
      },
      "source": [
        "#Retrain and test Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FirLiTyOYq5y"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qI4fLMYYq51",
        "outputId": "5b739ff7-f3f9-4771-8f1e-23345de4e4aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = LogisticRegression(solver='sag', penalty='l2', max_iter=1000, random_state=1)\n",
        "with joblib.parallel_backend('dask'):\n",
        "  lr.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = lr.predict(X_train.compute())\n",
        "preds_test = lr.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.8373569435745601\n",
            "Test score is:  0.7377746279234585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKbeX7zuZTrx"
      },
      "source": [
        "#Retrain and test KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57vtHA3EZTrx"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqxDXVm0ZTrz",
        "outputId": "67f0f073-9e6c-456f-d8bb-049aecb5db65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  knn.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = knn.predict(X_train.compute())\n",
        "preds_test = knn.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.7412372799224681\n",
            "Test score is:  0.5077160493827161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzbj0gKKZTr1"
      },
      "source": [
        "#Retrain and test SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCuLE5V7ZTr1"
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xJod6eZTr4",
        "outputId": "8d1dc225-2f71-4cc2-d7cd-91e960abd779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time_start = time.time()\n",
        "svc = svm.SVC(kernel = 'linear', random_state=1)\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  svc.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = svc.predict(X_train.compute())\n",
        "preds_test = svc.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))\n",
        "print('Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.9657265961613787\n",
            "Test score is:  0.4656895403064623\n",
            "Time elapsed: 55.67445993423462 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCWXn_0ZTr5"
      },
      "source": [
        "#Retrain and test Decision Tree model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECotlMslZTr6"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahzq3upaZTr7",
        "outputId": "101839ac-de38-4fc5-c2ac-83cfff33be9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dtc = DecisionTreeClassifier(\n",
        "     criterion='entropy',\n",
        "    max_features=5,\n",
        "    max_depth=5,\n",
        "    random_state=1 \n",
        ")\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  dtc.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = dtc.predict(X_train.compute())\n",
        "preds_test = dtc.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  0.9460527550924622\n",
            "Test score is:  0.6991228070175438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK6vVjW0ZTr9"
      },
      "source": [
        "#Retrain and test Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgJjzy8KZTr9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEJhJuoAZTr_",
        "outputId": "3c926052-bda3-40fe-b457-b6e9c50a3a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
        "with joblib.parallel_backend('dask'):\n",
        "  rfc.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = rfc.predict(X_train.compute())\n",
        "preds_test = rfc.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  1.0\n",
            "Test score is:  0.7288461538461537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOQQDGHjZTsA"
      },
      "source": [
        "#Retest and train Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxHrO4NDZTsB"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfuuLrSWZTsE",
        "outputId": "28b087af-e5b6-4a66-8799-614e0f14c5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=1000, \n",
        "    max_depth=5, \n",
        "    loss='deviance', \n",
        "    random_state=1\n",
        "    )\n",
        "with joblib.parallel_backend('dask'):\n",
        "  gbc.fit(X_train.compute(), y_train.compute())\n",
        "\n",
        "preds_train = gbc.predict(X_train.compute())\n",
        "preds_test = gbc.predict(X_test.compute())\n",
        "\n",
        "print(\"Training score is: \", roc_auc_score(preds_train, y_train.compute()))\n",
        "print(\"Test score is: \", roc_auc_score(preds_test, y_test.compute()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score is:  1.0\n",
            "Test score is:  0.7435897435897436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Gr2VVvoYEa"
      },
      "source": [
        "#Conclusion\n",
        "\n",
        "The longest model to run was th SVM model for both the PCA and non PCA variant at about 100 seconds.\n",
        "\n",
        "None of these models were tuned much after initial parameters were set, as this is a test of applying parellization to the model, and not creating optimal model accuracy.\n",
        "\n",
        "In terms of results:\n",
        "\n",
        "Prior to PCA, Decisision Tree, Rndom Forest, Gradient Boosting, and SVM all performed fantastically with over 96% accuracy on the test set.\n",
        "\n",
        "After PCA, Gradient Boosting showed good results with 74.35% test accuracy."
      ]
    }
  ]
}